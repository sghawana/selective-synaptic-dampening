(MUL) mtech1@auk:~/selective-synaptic-dampening/src$ ./cifar10_random_exps.sh 0 1

-----------------------------------------------------------------------------------------------------------------------
BASELINE
-----------------------------------------------------------------------------------------------------------------------
Files already downloaded and verified
Files already downloaded and verified
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose "Don't visualize my results"
wandb: Tracking run with wandb version 0.17.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/mtech1/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
89.40862274169922 89.40862274169922 tensor(0.7716) 0.7951807228915663
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Df ▁
wandb:           MIA ▁
wandb:    MethodTime ▁
wandb: RetainTestAcc ▁
wandb:       TestAcc ▁
wandb:           ZRF ▁
wandb:  model_scaler ▁
wandb: 
wandb: Run summary:
wandb:            Df 92.77108
wandb:           MIA 0.79518
wandb:    MethodTime 106.36902
wandb: RetainTestAcc 89.40862
wandb:       TestAcc 89.40862
wandb:           ZRF 0.7716
wandb:  model_scaler 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/mtech1/selective-synaptic-dampening/src/wandb/offline-run-20240814_123437-g24w5gn8
wandb: Find logs at: ./wandb/offline-run-20240814_123437-g24w5gn8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

-----------------------------------------------------------------------------------------------------------------------
SSD_TUNING
-----------------------------------------------------------------------------------------------------------------------
Files already downloaded and verified
Files already downloaded and verified
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose "Don't visualize my results"
wandb: Tracking run with wandb version 0.17.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
{'lower_bound': 1, 'exponent': 1, 'magnitude_diff': None, 'min_layer': -1, 'max_layer': -1, 'forget_threshold': 1, 'dampening_constant': 1, 'selection_weighting': 10}
/home/mtech1/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
62.40110778808594 62.40110778808594 tensor(0.8975) 0.43373493975903615
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Df ▁
wandb:           MIA ▁
wandb:    MethodTime ▁
wandb: RetainTestAcc ▁
wandb:       TestAcc ▁
wandb:           ZRF ▁
wandb:  model_scaler ▁
wandb: 
wandb: Run summary:
wandb:            Df 77.10843
wandb:           MIA 0.43373
wandb:    MethodTime 116.69959
wandb: RetainTestAcc 62.40111
wandb:       TestAcc 62.40111
wandb:           ZRF 0.89748
wandb:  model_scaler 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/mtech1/selective-synaptic-dampening/src/wandb/offline-run-20240814_123703-b95jy5lf
wandb: Find logs at: ./wandb/offline-run-20240814_123703-b95jy5lf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

-----------------------------------------------------------------------------------------------------------------------
FINETUNE
-----------------------------------------------------------------------------------------------------------------------
Files already downloaded and verified
Files already downloaded and verified
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose "Don't visualize my results"
wandb: Tracking run with wandb version 0.17.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Epoch [0], last_lr: 0.02000, train_loss: 0.2973, val_loss: 0.3654, val_acc: 87.6582
Epoch [1], last_lr: 0.02000, train_loss: 0.2064, val_loss: 0.3821, val_acc: 87.0352
Epoch [2], last_lr: 0.02000, train_loss: 0.1600, val_loss: 0.3944, val_acc: 87.2528
Epoch [3], last_lr: 0.02000, train_loss: 0.1144, val_loss: 0.4477, val_acc: 86.3034
Epoch [4], last_lr: 0.02000, train_loss: 0.0926, val_loss: 0.4495, val_acc: 86.6891
/home/mtech1/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
86.68907928466797 86.68907928466797 tensor(0.7776) 0.7469879518072289
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Df ▁
wandb:           MIA ▁
wandb:    MethodTime ▁
wandb: RetainTestAcc ▁
wandb:       TestAcc ▁
wandb:           ZRF ▁
wandb:  model_scaler ▁
wandb: 
wandb: Run summary:
wandb:            Df 91.56626
wandb:           MIA 0.74699
wandb:    MethodTime 170.39622
wandb: RetainTestAcc 86.68908
wandb:       TestAcc 86.68908
wandb:           ZRF 0.77763
wandb:  model_scaler 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/mtech1/selective-synaptic-dampening/src/wandb/offline-run-20240814_124029-b6gbt4x0
wandb: Find logs at: ./wandb/offline-run-20240814_124029-b6gbt4x0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

-----------------------------------------------------------------------------------------------------------------------
AMNESIAC
-----------------------------------------------------------------------------------------------------------------------
Files already downloaded and verified
Files already downloaded and verified
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose "Don't visualize my results"
wandb: Tracking run with wandb version 0.17.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Epoch [0], last_lr: 0.00010, train_loss: 0.2762, val_loss: 0.3614, val_acc: 87.6681
Epoch [1], last_lr: 0.00010, train_loss: 0.1875, val_loss: 0.3435, val_acc: 88.8944
Epoch [2], last_lr: 0.00010, train_loss: 0.1249, val_loss: 0.3897, val_acc: 88.2812
/home/mtech1/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
88.28125 88.28125 tensor(0.8782) 0.3373493975903614
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Df ▁
wandb:           MIA ▁
wandb:    MethodTime ▁
wandb: RetainTestAcc ▁
wandb:       TestAcc ▁
wandb:           ZRF ▁
wandb:  model_scaler ▁
wandb: 
wandb: Run summary:
wandb:            Df 73.49397
wandb:           MIA 0.33735
wandb:    MethodTime 143.19453
wandb: RetainTestAcc 88.28125
wandb:       TestAcc 88.28125
wandb:           ZRF 0.87818
wandb:  model_scaler 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/mtech1/selective-synaptic-dampening/src/wandb/offline-run-20240814_124406-fjje3dxq
wandb: Find logs at: ./wandb/offline-run-20240814_124406-fjje3dxq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

-----------------------------------------------------------------------------------------------------------------------
BLINDSPOT(TEACHER)
-----------------------------------------------------------------------------------------------------------------------
Files already downloaded and verified
Files already downloaded and verified
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose "Don't visualize my results"
wandb: Tracking run with wandb version 0.17.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
/home/mtech1/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
Epoch 1 Unlearning Loss 0.009212739765644073
88.4493637084961 88.4493637084961 tensor(0.8802) 0.5301204819277109
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Df ▁
wandb:           MIA ▁
wandb:    MethodTime ▁
wandb: RetainTestAcc ▁
wandb:       TestAcc ▁
wandb:           ZRF ▁
wandb:  model_scaler ▁
wandb: 
wandb: Run summary:
wandb:            Df 90.36144
wandb:           MIA 0.53012
wandb:    MethodTime 111.95002
wandb: RetainTestAcc 88.44936
wandb:       TestAcc 88.44936
wandb:           ZRF 0.88017
wandb:  model_scaler 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/mtech1/selective-synaptic-dampening/src/wandb/offline-run-20240814_124705-2yj2l8hp
wandb: Find logs at: ./wandb/offline-run-20240814_124705-2yj2l8hp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.

-----------------------------------------------------------------------------------------------------------------------
RETRAIN
-----------------------------------------------------------------------------------------------------------------------
Files already downloaded and verified
Files already downloaded and verified
wandb: (1) Create a W&B account
wandb: (2) Use an existing W&B account
wandb: (3) Don't visualize my results
wandb: Enter your choice: 3
wandb: You chose "Don't visualize my results"
wandb: Tracking run with wandb version 0.17.5
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Epoch [0], last_lr: 0.00997, train_loss: 0.6083, val_loss: 0.3354, val_acc: 88.5680
Epoch [1], last_lr: 0.01997, train_loss: 0.2487, val_loss: 0.4228, val_acc: 85.7002
/home/mtech1/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch [2], last_lr: 0.01000, train_loss: 0.1530, val_loss: 0.3575, val_acc: 88.6768
Epoch [3], last_lr: 0.01000, train_loss: 0.0966, val_loss: 0.3827, val_acc: 88.4197
Epoch [4], last_lr: 0.01000, train_loss: 0.0657, val_loss: 0.3914, val_acc: 88.5087
Epoch [5], last_lr: 0.01000, train_loss: 0.0457, val_loss: 0.3999, val_acc: 88.7856
Epoch [6], last_lr: 0.01000, train_loss: 0.0379, val_loss: 0.4095, val_acc: 88.7856
Epoch [7], last_lr: 0.01000, train_loss: 0.0292, val_loss: 0.4181, val_acc: 88.9933
Epoch [8], last_lr: 0.00200, train_loss: 0.0094, val_loss: 0.3527, val_acc: 90.5261
Epoch [9], last_lr: 0.00200, train_loss: 0.0045, val_loss: 0.3523, val_acc: 90.7140
Epoch [10], last_lr: 0.00200, train_loss: 0.0034, val_loss: 0.3552, val_acc: 90.5360
Epoch [11], last_lr: 0.00200, train_loss: 0.0030, val_loss: 0.3539, val_acc: 90.6052
Epoch [12], last_lr: 0.00040, train_loss: 0.0027, val_loss: 0.3566, val_acc: 90.5558
Epoch [13], last_lr: 0.00040, train_loss: 0.0025, val_loss: 0.3550, val_acc: 90.6448
Epoch [14], last_lr: 0.00040, train_loss: 0.0025, val_loss: 0.3534, val_acc: 90.6250
Epoch [15], last_lr: 0.00040, train_loss: 0.0024, val_loss: 0.3515, val_acc: 90.5360
Epoch [16], last_lr: 0.00008, train_loss: 0.0024, val_loss: 0.3550, val_acc: 90.5953
Epoch [17], last_lr: 0.00008, train_loss: 0.0024, val_loss: 0.3540, val_acc: 90.6349
Epoch [18], last_lr: 0.00008, train_loss: 0.0024, val_loss: 0.3569, val_acc: 90.5459
Epoch [19], last_lr: 0.00008, train_loss: 0.0023, val_loss: 0.3551, val_acc: 90.6052
/home/mtech1/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
90.605224609375 90.605224609375 tensor(0.7518) 0.6987951807228916
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:            Df ▁
wandb:           MIA ▁
wandb:    MethodTime ▁
wandb: RetainTestAcc ▁
wandb:       TestAcc ▁
wandb:           ZRF ▁
wandb:  model_scaler ▁
wandb: 
wandb: Run summary:
wandb:            Df 93.9759
wandb:           MIA 0.6988
wandb:    MethodTime 361.25347
wandb: RetainTestAcc 90.60522
wandb:       TestAcc 90.60522
wandb:           ZRF 0.75182
wandb:  model_scaler 1
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/mtech1/selective-synaptic-dampening/src/wandb/offline-run-20240814_124937-oaq8uxpa
wandb: Find logs at: ./wandb/offline-run-20240814_124937-oaq8uxpa/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.